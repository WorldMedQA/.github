# ğŸ‘‹ Welcome to WorldMedQA! ğŸŒ

<img src="logo.png" alt="worldmedqa_logo" width="200"/>

The WorldMedQA team is on a mission to elevate medical AI by refining the benchmarks used to evaluate vision and language models for healthcare.

### Why It Matters:
- **MedQA Datasets**: Medical knowledge is typically evaluated using MedQA datasets, consisting of multiple-choice questions from exams like the USMLE.
- **Big Data Training**: LLMs, like GPT, are trained on vast datasets, including medical content from sources like PubMed and scholarly articles. ğŸ“š

### The Gaps Weâ€™re Filling:
- ğŸ©º **Real-world Validity**: Existing datasets contain errors that can affect clinical relevance.
- ğŸŒ **Linguistic Diversity**: Many benchmarks lack proper representation of non-English languages.
- ğŸ–¼ï¸ **Imaging Data**: Most medical QA benchmarks don't account for multimodal (text + image) data.
- ğŸ•°ï¸ **Training Data Contamination**: Older datasets may overlap with LLM training corpora, leading to biased evaluation.

### Our First Release ğŸš€
Weâ€™ve launched our first dataset - **WorldMedQA-V** - to help bridge these gaps.

**WorldMedQA-V** is a multilingual, multimodal, clinically-validated dataset with 568 image-based medical QAs from Brazil, Israel, Japan, and Spain,
designed to evaluate vision and language models for healthcare.

It is available now on Hugging Face and GitHub:
- [Hugging Face: WorldMedQA/V](https://huggingface.co/datasets/WorldMedQA/V/)
- [GitHub: WorldMedQA/V](https://github.com/WorldMedQA/V)

Letâ€™s build more equitable, effective, and representative health AI together!
